<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>Interact with local models</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Interact with local models</h1>



<div id="intro" class="section level2">
<h2>Intro</h2>
<p><a href="https://github.com/kuvaus/LlamaGPTJ-chat">LlamaGPT-Chat</a>
is a command line chat application. It integrates with the LLM via C++.
This means that it will only work with LLM’s that have been in the same
language. There are few available today that are free to download and
use. Because they are in C++, the models are able to run locally on your
computer, even if there is no GPU available. Most of the models are
quite fast in their responses. By integrating with LlamaGPT-Chat, able
to access multiple types of LLM’s with only one additional back-end
within <code>chattr</code>.</p>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<p>To get LlamaGPT-Chat working you machine you will need two
things:</p>
<ol style="list-style-type: decimal">
<li><p>A version of LlamaGPT-Chat that works in your computer</p></li>
<li><p>An LLM file that works with with the chat program</p></li>
</ol>
<div id="install-llamagpt-chat" class="section level3">
<h3>Install LLamaGPT-Chat</h3>
<p>LlamaGPT-Chat will need a “compiled binary” that is specific to your
Operating System. For example, for Windows, a compiled binary should be
an <em>.exe</em> file.</p>
<p>The GitHub repository offers pre-compiled binaries that you can
download and use: <a href="https://github.com/kuvaus/LlamaGPTJ-chat/releases">Releases</a>.
Depending on the system’s security, the pre-compiled program may blocked
from running. If that is the case, you will need to “build” the program
in your computer. The instructions to do this are <a href="https://github.com/kuvaus/LlamaGPTJ-chat#download">here</a>.</p>
</div>
<div id="llm-file" class="section level3">
<h3>LLM file</h3>
<p>The GitHub repository contains a <a href="https://github.com/kuvaus/LlamaGPTJ-chat#gpt-j-llama-and-mpt-models">list
of compatible models</a>. Download at least one of the models, and make
note of where it was saved to in your computer.</p>
</div>
</div>
<div id="setup" class="section level2">
<h2>Setup</h2>
<p>To start, instruct <code>chattr</code> to switch the LLamaGPT
back-end using <code>chattr_use()</code></p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(chattr)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="fu">chattr_use</span>(<span class="st">&quot;llamagpt&quot;</span>)</span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a><span class="co">#&gt; ── chattr</span></span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a><span class="co">#&gt; • Provider: LlamaGPT</span></span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co">#&gt; • Path/URL: ~/LlamaGPTJ-chat/build/bin/chat</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a><span class="co">#&gt; • Model: ~/ggml-gpt4all-j-v1.3-groovy.bin</span></span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co">#&gt; • Label: GPT4ALL 1.3 (LlamaGPT)</span></span></code></pre></div>
<p>If this is the first time you use this interface, confirm that the
path of to the compiled program, and the model matches to what you have
in your machine. To check that use <code>chattr_defaults()</code></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">chattr_defaults</span>()</span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="co">#&gt; ── chattr ──────────────────────────────────────────────────────────────────────</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">#&gt; ── Defaults for: Default ──</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a><span class="co">#&gt; ── Prompt:</span></span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a><span class="co">#&gt; • Use the R language, the tidyverse, and tidymodels</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a><span class="co">#&gt; ── Model</span></span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a><span class="co">#&gt; • Provider: LlamaGPT</span></span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a><span class="co">#&gt; • Path/URL: ~/LlamaGPTJ-chat/build/bin/chat</span></span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co">#&gt; • Model: ~/ggml-gpt4all-j-v1.3-groovy.bin</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a><span class="co">#&gt; • Label: GPT4ALL 1.3 (LlamaGPT)</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a><span class="co">#&gt; ── Model Arguments:</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="co">#&gt; • threads: 4</span></span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a><span class="co">#&gt; • temp: 0.01</span></span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a><span class="co">#&gt; • n_predict: 1000</span></span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a><span class="co">#&gt; ── Context:</span></span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a><span class="co">#&gt; Max Data Files: 0</span></span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a><span class="co">#&gt; Max Data Frames: 0</span></span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a><span class="co">#&gt; ✖ Chat History</span></span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a><span class="co">#&gt; ✖ Document contents</span></span></code></pre></div>
<p>If either, or both, paths are incorrect, correct them by updating the
<code>path</code>, and <code>model</code> arguments in
<code>chattr_defaults()</code></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">chattr_defaults</span>(<span class="at">path =</span> <span class="st">&quot;[path to compiled program]&quot;</span>, <span class="at">model =</span> <span class="st">&quot;[path to model]&quot;</span>)</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#&gt; ── chattr ──────────────────────────────────────────────────────────────────────</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co">#&gt; ── Defaults for: Default ──</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co">#&gt; ── Prompt:</span></span>
<span id="cb3-8"><a href="#cb3-8" tabindex="-1"></a><span class="co">#&gt; • Use the R language, the tidyverse, and tidymodels</span></span>
<span id="cb3-9"><a href="#cb3-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-10"><a href="#cb3-10" tabindex="-1"></a><span class="co">#&gt; ── Model</span></span>
<span id="cb3-11"><a href="#cb3-11" tabindex="-1"></a><span class="co">#&gt; • Provider: LlamaGPT</span></span>
<span id="cb3-12"><a href="#cb3-12" tabindex="-1"></a><span class="co">#&gt; • Path/URL: [path to compiled program]</span></span>
<span id="cb3-13"><a href="#cb3-13" tabindex="-1"></a><span class="co">#&gt; • Model: [path to model]</span></span>
<span id="cb3-14"><a href="#cb3-14" tabindex="-1"></a><span class="co">#&gt; • Label: GPT4ALL 1.3 (LlamaGPT)</span></span>
<span id="cb3-15"><a href="#cb3-15" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-16"><a href="#cb3-16" tabindex="-1"></a><span class="co">#&gt; ── Model Arguments:</span></span>
<span id="cb3-17"><a href="#cb3-17" tabindex="-1"></a><span class="co">#&gt; • threads: 4</span></span>
<span id="cb3-18"><a href="#cb3-18" tabindex="-1"></a><span class="co">#&gt; • temp: 0.01</span></span>
<span id="cb3-19"><a href="#cb3-19" tabindex="-1"></a><span class="co">#&gt; • n_predict: 1000</span></span>
<span id="cb3-20"><a href="#cb3-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb3-21"><a href="#cb3-21" tabindex="-1"></a><span class="co">#&gt; ── Context:</span></span>
<span id="cb3-22"><a href="#cb3-22" tabindex="-1"></a><span class="co">#&gt; Max Data Files: 0</span></span>
<span id="cb3-23"><a href="#cb3-23" tabindex="-1"></a><span class="co">#&gt; Max Data Frames: 0</span></span>
<span id="cb3-24"><a href="#cb3-24" tabindex="-1"></a><span class="co">#&gt; ✖ Chat History</span></span>
<span id="cb3-25"><a href="#cb3-25" tabindex="-1"></a><span class="co">#&gt; ✖ Document contents</span></span></code></pre></div>
<p>So that you do not need to change those defaults every time you start
a new R session, use <code>chattr_defaults_save()</code>. That will
create a YAML file in your working directory. <code>chattr</code> will
use that file to override the defaults.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">chattr_defaults_save</span>()</span></code></pre></div>
<p>Use <code>chattr_test()</code> to confirm that the
<code>chattr</code> is able to communicate with LLamaGPT-Chat, and that
the model it will use with your R session is accessible</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">chattr_test</span>()</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="co">#&gt; ✔ Model started sucessfully</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co">#&gt; ✔ Model session closed sucessfully</span></span></code></pre></div>
</div>
<div id="model-arguments" class="section level2">
<h2>Model Arguments</h2>
<p>The arguments sent to the model can be modified in
<code>chattr_defaults()</code> by modifying the
<code>model_arguments</code> argument. It expects a list object. Here
are the arguments it sets by default:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a><span class="fu">chattr_defaults</span>()<span class="sc">$</span>model_arguments</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a><span class="co">#&gt; $threads</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a><span class="co">#&gt; [1] 4</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a><span class="co">#&gt; $temp</span></span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="co">#&gt; [1] 0.01</span></span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a><span class="co">#&gt; $n_predict</span></span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="co">#&gt; [1] 1000</span></span></code></pre></div>
<p>Here is an example of adding <code>batch_size</code> to the
defaults:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">chattr_defaults</span>(<span class="at">model_arguments =</span> <span class="fu">list</span>(<span class="at">batch_size =</span> <span class="dv">40</span>))</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="co">#&gt; ── chattr ──────────────────────────────────────────────────────────────────────</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a><span class="co">#&gt; ── Defaults for: Default ──</span></span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a><span class="co">#&gt; ── Prompt:</span></span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a><span class="co">#&gt; • Use the R language, the tidyverse, and tidymodels</span></span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co">#&gt; ── Model</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="co">#&gt; • Provider: LlamaGPT</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="co">#&gt; • Path/URL: [path to compiled program]</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="co">#&gt; • Model: [path to model]</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="co">#&gt; • Label: GPT4ALL 1.3 (LlamaGPT)</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a><span class="co">#&gt; ── Model Arguments:</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="co">#&gt; • batch_size: 40</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a><span class="co">#&gt; • threads: 4</span></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co">#&gt; • temp: 0.01</span></span>
<span id="cb7-20"><a href="#cb7-20" tabindex="-1"></a><span class="co">#&gt; • n_predict: 1000</span></span>
<span id="cb7-21"><a href="#cb7-21" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb7-22"><a href="#cb7-22" tabindex="-1"></a><span class="co">#&gt; ── Context:</span></span>
<span id="cb7-23"><a href="#cb7-23" tabindex="-1"></a><span class="co">#&gt; Max Data Files: 0</span></span>
<span id="cb7-24"><a href="#cb7-24" tabindex="-1"></a><span class="co">#&gt; Max Data Frames: 0</span></span>
<span id="cb7-25"><a href="#cb7-25" tabindex="-1"></a><span class="co">#&gt; ✖ Chat History</span></span>
<span id="cb7-26"><a href="#cb7-26" tabindex="-1"></a><span class="co">#&gt; ✖ Document contents</span></span></code></pre></div>
<p>To see the most current list of available model arguments go to: <a href="https://github.com/kuvaus/LlamaGPTJ-chat#detailed-command-list">Detailed
command list</a>.</p>
<p><strong>IMPORTANT</strong> - <code>chattr</code> passes the arguments
directly to LLamaGPT-chat as a command line flag, except
<code>model</code>. <code>chattr</code> will use the value in
<code>chattr_defaults(model = &quot;[path to model]&quot;)</code> instead.</p>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
